AI Agents in Production

I've built multiple AI agents now in active use. The primary one is an internal AEM Content Validator that streamlines medical editorial workflows. The system uses a multi-agent architecture built with LangChain and LangSmith for observability, orchestrating specialized validation agents that work in parallel to assess AEM Content Fragments across multiple dimensions. 

The architecture includes a Structure Validator Agent that uses LangChain's structured output capabilities with Pydantic models to validate content fragment schemas, hierarchy, and required field presence against AEM's GraphQL Content Fragment models. A Taxonomy Validator Agent leverages LangChain's tool-calling framework to query AEM's taxonomy APIs and verify that assigned tags, categories, and classification terms match the approved medical taxonomy (ICD-10, SNOMED CT, MeSH), checking for deprecated terms, misspellings, and hierarchical relationships. The Metadata Completeness Agent employs LangChain chains to analyze required vs. optional metadata fields, cross-reference with editorial guidelines, and identify missing SEO tags, publication dates, author information, and content relationships.

A Tone and Compliance Agent uses LangChain's prompt templates with medical editorial style guides to evaluate content tone, identify inappropriate language for medical audiences, check for unsubstantiated claims, and ensure compliance with FDA/HIPAA guidelines for medical content. This agent uses OpenAI's function calling to extract specific compliance violations and generate detailed violation reports. The Schema Compliance Agent validates content structure against JSON Schema definitions, ensuring field types, constraints, and nested object relationships match the AEM Content Fragment model definitions using LangChain's output parsers and structured validation.

All agents use OpenAI GPT-4 for analysis and leverage LangChain's AgentExecutor with custom tools that interface with AEM Headless APIs via GraphQL queries. The system implements a Scoring Orchestrator that aggregates results from all validation agents, weights them according to criticality (schema compliance weighted highest, tone weighted for sensitivity), and produces JSON-structured scoring output with component-level scores (0-100), pass/fail gates, and severity classifications. A Rewrite Suggestion Agent uses LangChain's LLM chains with few-shot examples to generate specific, actionable rewrite suggestions for each violation, providing before/after examples and referencing the specific editorial guideline that triggered the suggestion.

The entire workflow is traced using LangSmith, which provides detailed observability into agent reasoning paths, token usage per agent, latency metrics, and validation accuracy over time. LangSmith's feedback loops are integrated to continuously improve agent performance based on author corrections and approval rates. The backend uses LangGraph for complex multi-step validation workflows that require sequential dependencies (e.g., schema validation must pass before taxonomy validation), with conditional routing between agents based on validation results. The system produces detailed content-quality feedback reports that highlight issues at the field, fragment, and component levels, enabling authors to address problems systematically. The UI is built in React, with a Node/Python backend integrated into AEM Headless APIs via GraphQL, and is actively used to improve publishing turnaround by catching compliance issues before content reaches staging environments.

I also have three production agents deployed on Hugging Face Spaces:
https://huggingface.co/fareezaiahmed

Deep Research Agent — A multi-agent research pipeline built with the OpenAI Agents framework, orchestrating specialized agents in a sequential workflow. The system uses a Planner Agent to decompose research queries into structured search plans (typically 5 web searches with reasoning), a Search Agent that performs parallel web searches and produces concise summaries using the OpenAI Agents WebSearchTool, and a Writer Agent that synthesizes findings into comprehensive 5-10 page markdown reports (1000+ words) with outlines, summaries, and follow-up questions. The ResearchManager coordinates the pipeline with async task execution and streaming status updates. The system includes an Email Agent for automated report distribution via SendGrid. The UI is built with Gradio, uses OpenAI GPT-4o-mini across all agents, and leverages Pydantic models for structured outputs. The entire workflow is traced using OpenAI's native tracing system for observability.

Trading Floor Simulation — A multi-trader AI simulation platform built with the OpenAI Agents framework and Model Context Protocol (MCP) servers. The system runs four autonomous trading agents (each with distinct strategies: Patience, Bold, Systematic, and Crypto) that make independent investment decisions. Each trader has its own researcher agent connected via MCP servers for real-time market analysis. The platform integrates with Polygon API for market data, supports multiple LLM backends (GPT-4o-mini, DeepSeek, Gemini, Grok via OpenRouter), and provides real-time portfolio tracking with Plotly visualizations. The Gradio UI updates every 2-4 minutes with portfolio values, holdings, transactions, and activity logs. The background trading engine runs on configurable intervals (default 60 minutes) and executes parallel trading decisions asynchronously.

Engineering Team Crew — An autonomous software development pipeline built with CrewAI, orchestrating a team of five specialized agents that collaborate to build complete full-stack applications. The crew includes an Engineering Lead that designs system architecture and module specifications, a Backend Engineer that implements Python modules and Flask API servers with Docker-based safe code execution, a Frontend Engineer that builds React/Redux applications with Bootstrap styling, a Design Engineer that creates comprehensive UX/UI specifications with color schemes and typography, and a Test Engineer that writes unit tests, integration tests, and API tests. The system uses YAML-based configuration for agent roles and task definitions, enabling declarative workflow orchestration. The crew executes tasks sequentially, passing context between agents (e.g., design specifications inform code implementation, code informs testing). It produces production-ready outputs including complete Python modules, Flask APIs with deployment scripts, React applications with Redux state management, and comprehensive test suites. All agents use OpenAI GPT-4o and leverage CrewAI's built-in code execution capabilities with configurable timeouts and retry limits.

LangChain/LangSmith Usage Summary

I use LangChain and LangGraph extensively to build multi-agent systems with state-based workflows. In the Sidekick agent, I use LangGraph's StateGraph with TypedDict state management, conditional routing between worker and evaluator nodes, and ToolNode for tool execution with Playwright browser automation, file management, and web search via LangChain Community toolkits. I leverage ChatOpenAI with bind_tools() for function calling and with_structured_output() with Pydantic models for type-safe agent responses. For production systems like the AEM Content Validator, I use LangGraph for sequential validation workflows with conditional dependencies and LangSmith for tracing agent reasoning paths, token usage, latency metrics, and feedback loops that continuously improve performance over time. The architecture supports parallel validation agents, checkpointing with MemorySaver, and structured error handling through Pydantic validation, enabling complex multi-step agent orchestration with full observability.

